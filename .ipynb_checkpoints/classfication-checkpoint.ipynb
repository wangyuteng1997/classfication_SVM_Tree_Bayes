{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "data=pd.read_csv(\"train_values.csv\")\n",
    "label=pd.read_csv(\"train_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing the data\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "le = LabelEncoder()\n",
    "\n",
    "#use data_fit for the label of every label of the colone\n",
    "data_fit = data\n",
    "\n",
    "colone = list(data)\n",
    "for colone in colone:\n",
    "    data_fit[colone] = le.fit_transform(data[colone])\n",
    "#这里必须要将训练集里面的非数字特征 转为 数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data and label\n",
    "dataset = np.array(data_fit)\n",
    "label_data = np.array(label[\"damage_grade\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split test and train  (set)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test  = train_test_split(dataset, label_data, test_size=0.2, random_state=50) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  372  4596     0]\n",
      " [  501 29153     4]\n",
      " [  153 17336     6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.36      0.07      0.12      4968\n",
      "           2       0.57      0.98      0.72     29658\n",
      "           3       0.60      0.00      0.00     17495\n",
      "\n",
      "    accuracy                           0.57     52121\n",
      "   macro avg       0.51      0.35      0.28     52121\n",
      "weighted avg       0.56      0.57      0.42     52121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Gaussian Bayesian models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "#Set classifier\n",
    "clf=GaussianNB()\n",
    "#Training classifier\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 600 samples from randomly selected data because the svm algorithm is particularly slow\n",
    "data=pd.read_csv(\"train_values.csv\")\n",
    "data=data.sample(n=1000)\n",
    "label=pd.read_csv(\"train_labels.csv\")\n",
    "label=label.sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processing the data\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "#use data_fit for the label of every label of the colone\n",
    "data_fit = data\n",
    "\n",
    "colone = list(data)\n",
    "for colone in colone:\n",
    "    data_fit[colone] = le.fit_transform(data[colone])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(n_components=10)\n"
     ]
    }
   ],
   "source": [
    "#dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "print(pca)\n",
    "data_fit=pca.fit_transform(data_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data and label                           \n",
    "dataset = np.array(data_fit)\n",
    "label_data = np.array(label[\"damage_grade\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    587\n",
       "3    321\n",
       "1     92\n",
       "Name: damage_grade, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unbalanced data and label\n",
    "label['damage_grade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One of the simplest ways to generate a sample of unbalanced data is to generate a sample of minority classes.\n",
    "# turn into balanced data and set\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "x_resampled, y_resampled = ros.fit_resample(dataset, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 587), (2, 587), (3, 587)]\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print(sorted(collections.Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split test and train  (set)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test  = train_test_split(x_resampled, y_resampled, test_size=0.3, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 161), (2, 183), (3, 185)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(collections.Counter(y_test).items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100  53   8]\n",
      " [ 86  57  40]\n",
      " [ 89  63  33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.36      0.62      0.46       161\n",
      "           2       0.33      0.31      0.32       183\n",
      "           3       0.41      0.18      0.25       185\n",
      "\n",
      "    accuracy                           0.36       529\n",
      "   macro avg       0.37      0.37      0.34       529\n",
      "weighted avg       0.37      0.36      0.34       529\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "clf = SVC(kernel='linear')  \n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. accuracy 就是该模型训练的准确率\n",
    "2. macro avg 和 weighted avg 参数看下面的链接\n",
    "<https://blog.csdn.net/lyb3b3b/article/details/84819931>\n",
    "3. support表示在test集里面各类出现的次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear核\n",
    "主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想了。\n",
    "2. RBF核\n",
    "主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。通过训练数据的交叉验证来寻找合适的参数，验证过程比较耗时。\n",
    "<https://blog.csdn.net/weixin_40835491/article/details/89301721>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification_report函数\n",
    "> https://www.cnblogs.com/178mz/p/8558435.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use one hot code for the data\n",
    "data_one_hot = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data and label\n",
    "dataset = np.array(data_one_hot)\n",
    "label_data = np.array(label[\"damage_grade\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split test and train  (set)\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test  = train_test_split(dataset, label_data, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2173  2686   109]\n",
      " [ 1034 25059  3565]\n",
      " [  108  7582  9805]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.44      0.52      4968\n",
      "           2       0.71      0.84      0.77     29658\n",
      "           3       0.73      0.56      0.63     17495\n",
      "\n",
      "    accuracy                           0.71     52121\n",
      "   macro avg       0.70      0.61      0.64     52121\n",
      "weighted avg       0.71      0.71      0.70     52121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000,max_features=4)  # square root of number of features \n",
    "clf = clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tree\n",
    "> https://blog.csdn.net/qq_16633405/article/details/61200502\n",
    "***\n",
    "* n_estimators : integer, optional (default=10)\n",
    "  随机森林中树的个数，即学习器的个数。\n",
    "* max_features : (default=”auto”)\n",
    "  划分叶子节点，选择的最大特征数目"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
